{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Wrangling with Pandas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Processing the time series census data extracted from the Australian Bureau of Statistics Datapack 2011\n",
      "\n",
      "```2011 Datapacks BCP_IP_TSP_PEP_ECP_WPP_Release 3/2011 Time Series Profile Release 3/Long Descriptor/SA3```\n",
      "\n",
      "## Time series profile\n",
      "\n",
      "The Time Series Profile (TSP) consists of 34 tables containing key Census characteristics of persons, families and dwellings. Second release tables will contain Labour Force classifications which include occupation, industry and qualifications.\n",
      "\n",
      "The Time Series Profile presents data from three Censuses, based on the geographical boundaries for the most recent of the three. Comparing data from different Time Series Profiles is not valid, because geographical boundaries are subject to change between Censuses. Within a Time Series Profile, data for the two previous Censuses is concorded to the geographical boundaries for the most recent one.\n",
      "\n",
      "The 2011 Time Series Profile compares data from the 2001, 2006 and 2011 Censuses where the data classifications are comparable. If a data classification has been revised between Censuses, data will be output on the classification that has been used in the 2011 Census. Footnotes explain the correspondence between the data classifications of previous Censuses and the 2011 classification. The data are based on place of enumeration (where people were counted on Census night).\n",
      "\n",
      "The change in the geographical classification standards for the 2011 Census has a slight effect on Time Series Profiles. For example, the Statistical Local Area (SLA) used in previous Censuses is no longer available as a defined region in the new standard. As a transitional arrangement, and only for the 2011 Census, Time Series Profiles have been released at the SLA level. \n",
      "\n",
      "\n",
      "## SA3\n",
      "\n",
      "Statistical Areas Level 3 (SA3s) provide a standardised regional breakup of Australia. The aim of SA3s is to create a standard framework for the analysis of ABS data at the regional level through clustering groups of SA2s that have similar regional characteristics. SA3s are built from whole SA2s and in general have populations between 30,000 and 130,000 persons. They are often the functional areas of regional cities and large urban transport and service hubs.\n",
      "\n",
      "**Setting up the environment ([do not use pylab](http://carreau.github.io/posts/10-No-PyLab-Thanks.ipynb.html))**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats as sps\n",
      "import os\n",
      "\n",
      "pd.set_option('display.width', 4000)\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load the raw data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir = '../census-australia-abs2011-sa3/'\n",
      "readme_file = 'README.md'\n",
      "to_ignore = '2011Census_geog_desc_1st_and_2nd_release.csv' # This contains the locations\n",
      "file_list = os.listdir(data_dir)\n",
      "file_list.remove(readme_file)\n",
      "file_list.remove(to_ignore)\n",
      "\n",
      "# Read the first file to get region_id\n",
      "temp = pd.read_csv(data_dir + file_list[0])\n",
      "region_id = temp['region_id']\n",
      "\n",
      "# Read all files and join using region_id\n",
      "data = pd.DataFrame(region_id, columns=['region_id'])\n",
      "for sheet in file_list:\n",
      "    cur_data = pd.read_csv(data_dir + sheet)\n",
      "    data = data.merge(cur_data, how='outer', on='region_id')\n",
      "    \n",
      "# Remove duplicate columns\n",
      "data = data.T.groupby(level=0).first().T\n",
      "\n",
      "print('Number of regions: %d' % len(data.index))\n",
      "print('Number of features: %d' % len(data.columns))\n",
      "print('Some column headers...')\n",
      "print(data.columns.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Investigate data from 2011\n",
      "\n",
      "Find the features corresponding to the median or average values (summary statistics). Export this to a CSV file.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "year = '2011'\n",
      "data_2011 = data.filter(like=year)\n",
      "print('Shape of 2011 data')\n",
      "print(data_2011.shape)\n",
      "\n",
      "dataset = pd.concat([data_2011.filter(like='Median'), data_2011.filter(like='Average')], axis=1)\n",
      "print(dataset.columns)\n",
      "print(dataset.shape)\n",
      "dataset.to_csv('census_abs2011_summary.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most of the detailed data comes in the form of counts in various bins. We take a peek at the columns containing the words ```Rent_Total``` and construct a list of names that extract the counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_rent = data_2011.filter(like='Rent_Total')\n",
      "print(data_rent.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rent_steps = [1, 200, 300, 400, 600, 800, 1000, 1250, 1500, 2000, 2500, 3000]\n",
      "cols = []\n",
      "for ix in range(len(rent_steps)-1):\n",
      "    cols.append('2011_Census_%d_%d_Rent_Total' % (rent_steps[ix], rent_steps[ix+1]-1))\n",
      "cols.append('2011_Census_%d_or_more_Rent_Total' % rent_steps[-1])\n",
      "print(cols)\n",
      "data_2011[cols].ix[42].plot(kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Finding a single scalar to represent the buckets of counts\n",
      "\n",
      "For many (if not most) machine learning methods, it is convenient to have one single scalar for each feature. The census data provides finer grained information, and we would like to summarise it. **Note that the bucket sizes are not uniform.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _bins2median(steps, values):\n",
      "    \"\"\"Find the median of the buckets of counts\"\"\"\n",
      "    total = np.sum(values)\n",
      "    if total == 0:\n",
      "        return 0.0\n",
      "    middle_idx = total/2.\n",
      "    seen = 0\n",
      "    for ix,val in enumerate(values):\n",
      "        seen += val\n",
      "        if seen > middle_idx:\n",
      "            break\n",
      "    interp = (middle_idx-(seen-val))/float(val)\n",
      "    median = steps[ix-1] + interp*(steps[ix]-steps[ix-1])\n",
      "    return median\n",
      "\n",
      "def bins2median(steps, columns):\n",
      "    \"\"\"Find the median of each row\"\"\"\n",
      "    median = []\n",
      "    for ix in range(columns.shape[0]):\n",
      "        cur_row = np.array(columns.loc[ix,:])\n",
      "        median.append(_bins2median(steps, cur_row))\n",
      "    return np.array(median)\n",
      "\n",
      "df = pd.DataFrame(region_id, columns=['region_id'])\n",
      "median_rent = bins2median(rent_steps, data[cols])\n",
      "df['median_rent'] = median_rent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We compare our median estimate with the reported median weekly rent."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_week = np.array(data['Median_rent_weekly_Census_year_2011'])\n",
      "width = 0.35 # width of the bars\n",
      "plot_idx = np.arange(81,129)\n",
      "fig = plt.figure(figsize=(16,5))\n",
      "ax = fig.add_subplot(111)\n",
      "ax.bar(plot_idx-width, y_week[plot_idx], width, color='r', label='weekly')\n",
      "ax.bar(plot_idx, median_rent[plot_idx], width, color='b', label='est. median')\n",
      "ax.legend()\n",
      "ax.set_xticks(plot_idx)\n",
      "ax.set_xticklabels(np.array(data['region_id'], dtype=int)[plot_idx], rotation=90)\n",
      "ax.set_xlabel('Region ID')\n",
      "ax.set_ylabel('AUD')\n",
      "\n",
      "print(\"Pearson's linear correlation = %1.4f\" % sps.pearsonr(y_week, median_rent)[0])\n",
      "print(\"Spearman's rank correlation  = %1.4f\" % sps.spearmanr(y_week, median_rent)[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other examples of feature construction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cols = ['Total_persons_2011_Census_Males','Total_persons_2011_Census_Females']\n",
      "df[cols] = data_2011[cols]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "age_steps = [0, 5, 15, 20, 25, 35, 45, 55, 65, 75, 85]\n",
      "cols = []\n",
      "for ix in range(len(age_steps)-1):\n",
      "    cols.append('Age_group_%d_%d_years_2011_Census_Females' % (age_steps[ix], age_steps[ix+1]-1))\n",
      "cols.append('Age_group_%d_years_and_over_2011_Census_Females' % age_steps[-1])\n",
      "print(cols)\n",
      "median_age_female = bins2median(age_steps, data[cols])\n",
      "df['median_age_female'] = median_age_female\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cols = []\n",
      "for ix in range(len(age_steps)-1):\n",
      "    cols.append('Age_group_%d_%d_years_2011_Census_Males' % (age_steps[ix], age_steps[ix+1]-1))\n",
      "cols.append('Age_group_%d_years_and_over_2011_Census_Males' % age_steps[-1])\n",
      "print(cols)\n",
      "median_age_male = bins2median(age_steps, data[cols])\n",
      "df['median_age_male'] = median_age_male\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}