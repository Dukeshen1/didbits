{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation of vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment ([do not use pylab](http://carreau.github.io/posts/10-No-PyLab-Thanks.ipynb.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import os\n",
    "\n",
    "pd.set_option('display.width', 4000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../DataWrangling/census_abs2011_summary.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise to unit interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalise_01(data):\n",
    "    minimum = np.min(data)\n",
    "    maximum = np.max(data)\n",
    "    return (data-minimum)/(maximum-minimum)\n",
    "\n",
    "normalise_01(raw_data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero mean and unit variance (z normalisation)\n",
    "\n",
    "This standardises the features to all have zero mean and similar (unit) spread. Note that this does not mean that the data is converted into a Gaussian distribution, it just shifts and scales each feature.\n",
    "$$\n",
    "x_{normalised} = \\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation. In this case, $\\mu$ and $\\sigma$ are estimated from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalise_z(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    assert np.any(sigma > 0.0), 'Zero variance'\n",
    "    return (data-mu)/sigma\n",
    "\n",
    "normalise_z(raw_data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-Cox transform\n",
    "\n",
    "The [Box Cox transform](http://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation) converts data into a normal (Gaussian distribution). It is given by\n",
    "$$\n",
    "x_{new} =\n",
    "\\begin{cases}\n",
    "\\frac{x^\\lambda - 1}{\\lambda}& \\lambda\\neq 0\\\\\n",
    "\\ln(x)&\\lambda = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Finding the parameter $\\lambda$ is done by maximising the log likelihood.\n",
    "\n",
    "The code below is updated from [WeatherGod at github](https://github.com/WeatherGod/NNforZR/blob/master/boxcox.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def boxcox_auto(x):\n",
    "    \"\"\"\n",
    "    Automatically determines the lambda needed to perform a boxcox\n",
    "    transform of the given vector of data points.  Note that it will\n",
    "    also automatically offset the datapoints so that the minimum value\n",
    "    is just above 0 to satisfy the criteria for the boxcox transform.\n",
    "    \n",
    "    The object returned by this function contains the transformed data\n",
    "    ('bc_data'), the lambda ('lamb'), and the offset used on the data\n",
    "    points ('dataOffset').  This object can be fed easily into\n",
    "    boxcox_inverse() to retrieve the original data values like so:\n",
    "    \n",
    "    EXAMPLE:\n",
    "    >>> bc_results = boxcox_auto(data_vector)\n",
    "    >>> print bc_results\n",
    "    {'lamb': array([ 0.313]), 'bc_data': array([ 0.47712018,  1.33916353,  6.66393874, ...,  3.80242394,\n",
    "    3.79166974,  0.47712018]), 'data_offset': 2.2204460492503131e-16}\n",
    "    >>> reconstit = boxcox_inverse(**bc_results)\n",
    "    >>> print numpy.mean((dataVector - reconstit) ** 2)\n",
    "    5.6965875916e-29\n",
    "    \"\"\"\n",
    "    const_offset = -np.min(x) + 1e-8\n",
    "    tempX = x + const_offset\n",
    "    bclambda = sp.optimize.fmin(boxcox_opt, 0.0, args=(tempX), maxiter=2000, disp=0)\n",
    "    bclambda = bclambda[0]\n",
    "\n",
    "    # Generate the transformed data using the optimal lambda.\n",
    "    return({'bc_data': boxcox(tempX, bclambda), 'lamb': bclambda, 'data_offset': const_offset})\n",
    "\n",
    "def boxcox(x, lamb):\n",
    "    \"\"\"\n",
    "    boxcox() performs the boxcox transformation upon the data vector 'x',\n",
    "    using the supplied lambda value 'lamb'.\n",
    "    Note that this function does not check for minimum value of the data,\n",
    "    and it will not correct for values being below 0.\n",
    " \n",
    "    The function returns a vector the same size of x containing the\n",
    "    the transformed values.\n",
    "    \"\"\"\n",
    "    if (lamb != 0.0) :\n",
    "        return(((x ** lamb) - 1) / lamb)\n",
    "    else:\n",
    "        return(np.log(x))\n",
    "    \n",
    "def boxcox_inverse(bc_data, lamb, data_offset = 0.0) :\n",
    "    \"\"\"\n",
    "    Performs the inverse operation of the boxcox transform.\n",
    "    Note that one can use the output of boxcox_auto() to easily\n",
    "    run boxcox_inverse:\n",
    "    \n",
    "    >>> bc_results = boxcox_auto(data)\n",
    "    >>> reconstit_data = boxcox_inverse(**bc_results)\n",
    "    \n",
    "    Also can be used directly like so:\n",
    "    >>> trans_data = boxcox(orig_data, lamb_val)\n",
    "    >>> trans_data = do_processing(trans_data)\n",
    "    >>> reconstit_data = boxcox_inverse(trans_data, lamb_val)\n",
    "    \"\"\"\n",
    "    if (lamb != 0.0) :\n",
    "        return((((bc_data * lamb) + 1) ** (1.0/lamb)) - data_offset)\n",
    "    else :\n",
    "        return(np.exp(bc_data) - data_offset)\n",
    "\n",
    "\n",
    "def boxcox_opt(lamb, *pargs):\n",
    "    \"\"\"\n",
    "    Don't call this function, it is meant to be\n",
    "    used by boxcox_auto().\n",
    "    \"\"\"\n",
    "    x = np.array(pargs)\n",
    "    \n",
    "    # Transform data using a particular lambda.\n",
    "    xhat = boxcox(x, lamb[0])\n",
    "    \n",
    "    # The algorithm calls for maximizing the LLF; however, since we have\n",
    "    # only functions that minimize, the LLF is negated so that we can \n",
    "    # minimize the function instead of maximixing it to find the optimum lambda.\n",
    "    return(-(-(len(x)/2.0) * np.log(np.std(xhat.T)**2) + (lamb[0] - 1.0)*(np.sum(np.log(x)))))\n",
    "\n",
    "n_data = boxcox_auto(np.array(raw_data.values))\n",
    "pd.DataFrame(n_data['bc_data']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile normalisation\n",
    "\n",
    "Quantile normalisation converts the empirical distribution to [match a given distribution](http://en.wikipedia.org/wiki/Quantile_normalization).\n",
    "\n",
    "It is popular in [bioinformatics](http://bioinformatics.oxfordjournals.org/content/19/2/185) for normalising microarray data, where instead of using an arbitrary distribution, it uses the average empirical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quantile_normalise(data, avg=np.mean):\n",
    "    \"\"\"Perform quantile normalisation on the data (assumed to be a 2D array).\n",
    "    Assume each column is an example, and quantile normalise the rows.\n",
    "\n",
    "    Replace value with the mean of the values of the same rank.\n",
    "    \"\"\"\n",
    "    sort_idx = np.argsort(data, axis=1)\n",
    "    new_data = np.zeros(data.shape)\n",
    "    for idx in range(data.shape[1]):\n",
    "        new_val = avg(data[sort_idx==idx])\n",
    "        new_data[sort_idx == idx] = new_val\n",
    "    return new_data\n",
    "\n",
    "n_data = quantile_normalise(raw_data.values)\n",
    "pd.DataFrame(n_data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _score2rank(orig_scores):\n",
    "    \"\"\"Convert an array of scores into an array of normalised ranks,\n",
    "    such that the highest score has the highest rank (1/num_ex).\"\"\"\n",
    "    scores = orig_scores.copy()\n",
    "    idx_sort = np.argsort(scores)[::-1]\n",
    "    unsort = np.argsort(idx_sort)\n",
    "    scores = scores[idx_sort]\n",
    "\n",
    "    ranks = infer_ranks(scores)\n",
    "    \n",
    "    assert len(ranks) == len(scores), 'Shape mismatch'\n",
    "    ranks = ranks/(len(scores)+1.0)\n",
    "    ranks = ranks[unsort]\n",
    "    return ranks\n",
    "\n",
    "def score2rank(orig_data):\n",
    "    \"\"\"Convert each column of scores into normalised ranks\"\"\"\n",
    "    ranks = np.zeros(orig_data.shape)\n",
    "    for ix in range(orig_data.shape[1]):\n",
    "        ranks[:,ix] = _score2rank(orig_data[:,ix])\n",
    "    return ranks\n",
    "        \n",
    "\n",
    "def infer_ranks(scores):\n",
    "    \"\"\"impute the ranks of the scores, taking care of ties.\"\"\"\n",
    "    num_ex = len(scores)\n",
    "    unique_scores = np.unique(scores)\n",
    "    sc = scores.copy()\n",
    "    sc = sc[::-1]\n",
    "    left = num_ex - sc.searchsorted(unique_scores, side='right')\n",
    "    right = num_ex - sc.searchsorted(unique_scores)\n",
    "    ranks = np.zeros(num_ex)\n",
    "    for idx in range(len(unique_scores)):\n",
    "        cur_rank = 0.5*(left[idx]+right[idx]+1)\n",
    "        ranks[left[idx]:right[idx]] = cur_rank\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def normalise_rank(data):\n",
    "    \"\"\"Return rank normalised data in the interval [-0.5, 0.5]\"\"\"\n",
    "    return (1.-score2rank(data)) - 0.5\n",
    "\n",
    "def gauss_dist(data):\n",
    "    \"\"\"Return a Gaussian distributed dataset,\n",
    "    with zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    X = 1.-score2rank(data)\n",
    "    D = sps.norm()\n",
    "    return D.ppf(X)\n",
    "\n",
    "n_data = normalise_rank(raw_data.values)\n",
    "pd.DataFrame(n_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
